Metadata-Version: 2.1
Name: redun
Version: 0.16.2
Summary: Yet another redundant workflow engine.
Home-page: https://github.com/insitro/redun/
Author: Matt Rasmussen
Author-email: rasmus@insitro.com
Requires-Python: >= 3.7
License-File: LICENSE
Requires-Dist: aiobotocore[awscli,boto3] >=2.0.1
Requires-Dist: aiohttp <4,>=3.7.4
Requires-Dist: alembic >=1.4
Requires-Dist: boto3 >=1.16.63
Requires-Dist: botocore !=1.28.0,>=1.22.8
Requires-Dist: fancycompleter >=0.9.1
Requires-Dist: gcsfs >=2021.4.0
Requires-Dist: s3fs >=2021.11.1
Requires-Dist: sqlalchemy <2.1,>=1.4.0
Requires-Dist: python-dateutil >=2.8
Requires-Dist: pyyaml !=5.4.0,!=5.4.1,!=6.0.0
Requires-Dist: requests >=2.27.1
Requires-Dist: rich >=13.3.5
Requires-Dist: textual >=0.24.1
Requires-Dist: dataclasses >=0.8 ; python_version <= "3.6"
Requires-Dist: types-dataclasses >=0.6.6 ; python_version <= "3.6"
Provides-Extra: glue
Requires-Dist: pandas ; extra == 'glue'
Requires-Dist: pyarrow ; extra == 'glue'
Requires-Dist: pyspark ; extra == 'glue'
Provides-Extra: google-batch
Requires-Dist: google-cloud-batch >=0.2.0 ; extra == 'google-batch'
Requires-Dist: google-cloud-compute >=1.11.0 ; extra == 'google-batch'
Provides-Extra: k8s
Requires-Dist: kubernetes >=22.6 ; extra == 'k8s'
Provides-Extra: postgres
Requires-Dist: psycopg2 >=2.8 ; extra == 'postgres'
Provides-Extra: viz
Requires-Dist: pygraphviz ; extra == 'viz'


redun aims to be a more expressive and efficient workflow framework, built on
top of the popular Python programming language. It takes the somewhat contrarian
view that writing dataflows directly is unnecessarily restrictive, and by doing
so we lose abstractions we have come to rely on in most modern high-level
languages (control flow, compositiblity, recursion, high order functions, etc).
redun's key insight is that workflows can be expressed as lazy expressions, that
are then evaluated by a scheduler which performs automatic parallelization,
caching, and data provenance logging.

redun's key features are:

- Workflows are defined by lazy expressions that when evaluated emit dynamic directed acyclic
  graphs (DAGs), enabling complex data flows.
- Incremental computation that is reactive to both data changes as well as code changes.
- Workflow tasks can be executed on a variety of compute backend (threads, processes, AWS batch
  jobs, Spark jobs, etc).
- Data changes are detected for in memory values as well as external data sources such as files
  and object stores using file hashing.
- Code changes are detected by hashing individual Python functions and comparing against
  historical call graph recordings.
- Past intermediate results are cached centrally and reused across workflows.
- Past call graphs can be used as a data lineage record and can be queried for debugging and
  auditing.
    
